# Processamento de Fluxos Discretos e Contínuos de Dados

## Informações da Disciplina

- **Curso:** Engenharia de Dados
- **Instituição:** PUC Minas
- **Código:** PRO
- **Modalidade:** Ensino a Distância (EAD)

## Objetivos

Esta disciplina visa capacitar os estudantes para:

- **Processamento de Big Data:** Dominar técnicas avançadas de processamento de grandes volumes de dados utilizando Apache Spark
- **Streaming de Dados:** Implementar soluções para processamento de fluxos contínuos de dados em tempo real
- **Sistemas de Recomendação:** Desenvolver e implementar sistemas de recomendação escaláveis usando técnicas de machine learning
- **Arquiteturas Distribuídas:** Compreender e aplicar arquiteturas como Lambda para processamento híbrido de dados
- **Ferramentas de Mercado:** Utilizar tecnologias como Apache Kafka e Apache Flink para streaming de dados

## Metodologia

O aprendizado é estruturado através de:

- **Aulas Teóricas:** Fundamentos de processamento de fluxos discretos e contínuos
- **Práticas Hands-on:** Implementação com Apache Spark, Kafka e Flink
- **Projetos Aplicados:** Desenvolvimento de sistemas de recomendação e APIs
- **Seminários:** Revisão e discussão de tecnologias emergentes
- **Tarefas Práticas:** Exercícios para consolidação do conhecimento

## Ementa

A disciplina abrange os seguintes tópicos fundamentais:

- Introdução aos Fluxos Contínuos e Arquitetura Lambda
- Apache Spark: RDDs, processamento distribuído e aplicações em Big Data
- Sistemas de Recomendação com Apache Spark (ALS - Alternating Least Squares)
- Apache Kafka: Streaming de dados e processamento em tempo real
- Apache Flink: Processamento de fluxos avançado e analytics em tempo real
- Desenvolvimento de APIs para sistemas de recomendação
- Integração de tecnologias para soluções completas de dados

## Conteúdo Programático

### Módulo 1 - Fundamentos e Arquitetura Lambda
**Introdução aos Conceitos Fundamentais**
- **1.1** Conceitos de fluxos discretos vs. contínuos
- **1.2** Arquitetura Lambda: batch, speed e serving layers
- **1.3** Casos de uso e cenários de aplicação

### Módulo 2 - Apache Spark
**Processamento Distribuído de Big Data**
- **2.1** Introdução ao Apache Spark e RDDs (Resilient Distributed Datasets)
- **2.2** Operações em Spark: transformações e ações
- **2.3** Spark SQL e DataFrames
- **2.4** Machine Learning com Spark MLlib
- **2.5** Sistemas de Recomendação com ALS

### Módulo 3 - Apache Kafka
**Streaming de Dados em Tempo Real**
- **3.1** Fundamentos do Apache Kafka
- **3.2** Producers, Consumers e Topics
- **3.3** Configuração e administração de clusters Kafka
- **3.4** Integração com outras ferramentas de Big Data

### Módulo 4 - Apache Flink
**Processamento Avançado de Fluxos**
- **4.1** Introdução ao Apache Flink
- **4.2** DataStreams e processamento de eventos
- **4.3** Windowing e agregações em tempo real
- **4.4** Tolerância a falhas e state management

### Módulo 5 - Projeto Integrador
**Desenvolvimento de Sistema Completo**
- **5.1** Desenvolvimento de API para sistema de recomendação
- **5.2** Integração Spark + Kafka + Flink
- **5.3** Deploy e monitoramento de soluções
- **5.4** Otimização de performance

## Ferramentas e Tecnologias

Durante o curso, você trabalhará com:

- **Apache Spark:** Processamento distribuído e machine learning
- **Apache Kafka:** Streaming de dados e messaging
- **Apache Flink:** Processamento de fluxos em tempo real
- **Python:** Linguagem principal para desenvolvimento de APIs e scripts
- **Jupyter Notebooks:** Ambiente para experimentação e prototipagem
- **Docker:** Containerização de aplicações
- **Datasets:** MovieLens para sistemas de recomendação

## Projetos e Avaliações

### Atividades Práticas
- **Prática API Python Recomendação:** Desenvolvimento de API para sistema de recomendação
- **Prática Kafka:** Implementação de pipeline de streaming com Kafka
- **Prática Flink:** Processamento de fluxos em tempo real
- **Projeto Spark:** Projeto completo de tratamento de recomendações com streaming

### Seminários
- **Revisão de Apache Spark:** Apresentação e discussão de conceitos avançados

## Recursos Disponíveis

- **Códigos Práticos:** Exemplos implementados em Spark
- **Datasets:** sample_movielens_ratings.txt para práticas
- **Notebooks:** ExemploALS.ipynb com implementação de sistema de recomendação
- **APIs:** Código base (api.zip) para desenvolvimento de soluções

## Competências Desenvolvidas

Ao concluir esta disciplina, você será capaz de:

- Projetar e implementar arquiteturas de processamento de Big Data
- Desenvolver sistemas de recomendação escaláveis
- Implementar pipelines de streaming de dados em tempo real
- Utilizar Apache Spark para processamento distribuído
- Configurar e operar clusters Apache Kafka
- Desenvolver aplicações com Apache Flink
- Integrar múltiplas tecnologias para soluções completas de dados
- Desenvolver APIs robustas para sistemas de recomendação
- Otimizar performance em ambientes distribuídos

## Pré-requisitos

- Conhecimentos básicos em Python
- Fundamentos de bancos de dados
- Conceitos básicos de machine learning
- Familiaridade com ambientes Linux/Unix

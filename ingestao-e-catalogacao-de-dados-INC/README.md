# Ingestão e Catalogação de Dados

## Informações da Disciplina

- **Curso:** Engenharia de Dados
- **Instituição:** PUC Minas
- **Carga Horária:** 36 horas
- **Modalidade:** Ensino a Distância (EAD)

## Objetivos

Esta disciplina visa capacitar os estudantes para:

- **Fundamentos de Dados:** Compreender os conceitos de dado, informação, conhecimento e sabedoria, estabelecendo as bases para trabalhar com Big Data e Business Intelligence
- **Tecnologias de Ingestão:** Dominar tecnologias de ingestão de dados em batch e streaming, utilizando ferramentas modernas do mercado
- **Infraestrutura em Nuvem:** Implementar e gerenciar infraestrutura como código em plataformas de computação em nuvem, especialmente Microsoft Azure
- **Processos ETL/ELT:** Desenvolver e automatizar processos de extração, transformação e carregamento de dados utilizando Apache Airflow e Apache Kafka
- **Catalogação e Governança:** Implementar soluções de catalogação, validação e visualização de dados para garantir qualidade e governança

## Metodologia

O aprendizado é estruturado através de:

- **Material Didático Completo:** Disponibilizado na plataforma AVA da PUCMINAS
- **Videoaulas Explicativas:** Cobertura teórica e prática dos conceitos de ingestão e catalogação
- **Ferramentas de Mercado:** Implementação prática com Apache Airflow, Apache Kafka, Dremio e Microsoft Azure
- **Orientações Práticas:** Vídeos demonstrativos sobre instalação, configuração e operação das tecnologias
- **Projetos Hands-on:** Desenvolvimento de pipelines de dados completos em ambiente de nuvem

## Ementa

A disciplina abrange os seguintes tópicos fundamentais:

- Fundamentos de Dados e Business Intelligence
- Tecnologias de Big Data e Ingestão de Dados
- Infraestrutura como Código e Computação em Nuvem
- Arquiteturas de Data Lake e Organização de Dados
- Processos ETL/ELT em Batch e Streaming
- Apache Airflow para Orquestração de Dados
- Apache Kafka para Streaming de Dados
- Catalogação e Governança de Dados com Dremio

## Conteúdo Programático

### Aula 1 - Introdução e Configurações Iniciais
**Fundamentos e Conceitos Base**
- **1.1** Hierarquia: Dado, informação, conhecimento e sabedoria
- **1.2** Conceitos fundamentais de ingestão de dados
- **1.3** Business Intelligence: definições e aplicações
- **1.4** Big Data: características e desafios
- **1.5** Comparativo: Big Data x Business Intelligence
- **1.6** Tecnologias de ingestão em Batch e Streaming
- **1.7** Infraestrutura como Código e computação em nuvem
- **1.8** Criação de ambiente em Microsoft Azure

### Aula 2 - Criação e Configuração de Recursos em Nuvem
**Arquitetura e Infraestrutura**
- **2.1** Proposta de arquitetura de recursos em nuvem
- **2.2** Validação e configuração de serviços Azure
- **2.3** Criação de camadas de armazenamento de dados
- **2.4** Planejamento e organização de dados em Data Lake
- **2.5** Implementação prática de ingestão no Data Lake
- **2.6** Configuração de máquinas virtuais e Docker
- **2.7** Análise e otimização de custos em nuvem

### Aula 3 - Coleta, Tratamento e Normalização de Dados
**Qualidade e Preparação de Dados**
- **3.1** Estratégias de coleta de dados
- **3.2** Manipulação avançada de dados: formatos, encoding e delimitadores
- **3.3** Tratamento de campos textuais e caracteres especiais
- **3.4** Mecanismos de inferência de tipagem e Schema Assert
- **3.5** Técnicas de normalização de dados
- **3.6** Estratégias de atualização de dados
- **3.7** Change Data Capture (CDC) e atualizações temporais/sequenciais

### Aula 4 - Processos ETL & ELT (BATCH)
**Orquestração e Automação**
- **4.1** Fluxos de tarefas e fluxos de dados
- **4.2** Comparativo ETL vs ELT e ferramentas disponíveis
- **4.3** Apache Airflow: componentes e arquitetura
- **4.4** Conceitos e implementação de DAGs
- **4.5** Instalação e configuração do Apache Airflow
- **4.6** Desenvolvimento prático de DAGs
- **4.7** Agendamento e validação de execução de tarefas

### Aula 5 - Processos ETL & ELT (STREAMING)
**Processamento de Dados em Tempo Real**
- **5.1** Fundamentos de streaming de dados
- **5.2** Apache Kafka: arquitetura e componentes
- **5.3** Instalação e configuração do Apache Kafka
- **5.4** Operações práticas: criação de tópicos, produção e consumo de mensagens
- **5.5** kSQLDB: criação de streams e tabelas
- **5.6** Integração Python com Apache Kafka
- **5.7** Desenvolvimento de aplicações de streaming

### Aula 6 - Catalogação, Validação e Visualização de Dados
**Governança e Descoberta de Dados**
- **6.1** Conceitos de catálogo de dados e sua importância
- **6.2** Ferramentas de catalogação disponíveis no mercado
- **6.3** Dremio: requisitos de sistema e componentes
- **6.4** Instalação e configuração do Dremio
- **6.5** Gerenciamento de usuários e fontes de dados
- **6.6** Criação de espaços de usuários e consultas
- **6.7** Implementação de soluções de visualização

## Ferramentas e Tecnologias

Durante o curso, você trabalhará com:

- **Plataforma de Nuvem:** Microsoft Azure (Data Lake, Virtual Machines, Storage Accounts)
- **Orquestração:** Apache Airflow para pipelines batch
- **Streaming:** Apache Kafka e kSQLDB para dados em tempo real
- **Catalogação:** Dremio para descoberta e governança de dados
- **Containerização:** Docker para deployment de aplicações
- **Linguagens:** Python para desenvolvimento de pipelines
- **Infraestrutura:** Infraestrutura como Código (IaC)

## Competências Desenvolvidas

Ao concluir esta disciplina, você será capaz de:

- Projetar e implementar arquiteturas modernas de ingestão de dados
- Desenvolver pipelines de dados robustos tanto em batch quanto em streaming
- Configurar e gerenciar infraestrutura de dados em nuvem
- Implementar processos ETL/ELT automatizados e escaláveis
- Utilizar Apache Airflow para orquestração de workflows complexos
- Trabalhar com Apache Kafka para processamento de dados em tempo real
- Estabelecer práticas de catalogação e governança de dados
- Otimizar custos e performance em ambientes de nuvem
- Integrar diferentes tecnologias para criar soluções completas de engenharia de dados

## Pré-requisitos

- Conhecimentos básicos de programação (Python recomendado)
- Fundamentos de bancos de dados
- Conceitos básicos de sistemas distribuídos
- Familiaridade com linha de comando/terminal
